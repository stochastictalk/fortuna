{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First initialize datasets. These are used for training and evaluating the model.\n",
    "\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "\n",
    "from fortuna import ObjectDetectionDataset\n",
    "from fortuna.data import pedestrians_dirpath\n",
    "import fortuna.torchutils.transforms as T\n",
    "\n",
    "# Specify paths to images and masks.\n",
    "image_paths = sorted((pedestrians_dirpath / \"Images\").rglob(\"*\"))\n",
    "mask_paths = sorted((pedestrians_dirpath / \"Masks\").rglob(\"*\"))\n",
    "\n",
    "# We're going to use a random subsample of the dataset to validate the model.\n",
    "n = 10 # len(image_paths)\n",
    "training_fraction = 0.9\n",
    "training_ix = sample(range(n), int(training_fraction*n))\n",
    "validation_ix = list(set(range(n)) - set(training_ix))\n",
    "\n",
    "# Create ObjectDetectionDatasets for training and validation.\n",
    "training_dataset = ObjectDetectionDataset(\n",
    "    image_paths=[image_paths[j] for j in training_ix],\n",
    "    mask_paths=[mask_paths[j] for j in training_ix],\n",
    "    transforms=[T.RandomHorizontalFlip(0.5)]\n",
    ")\n",
    "validation_dataset = ObjectDetectionDataset(\n",
    "    image_paths=[image_paths[j] for j in validation_ix],\n",
    "    mask_paths=[mask_paths[j] for j in validation_ix]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [0/5]  eta: 0:00:42  lr: 0.000251  loss: 3.3684 (3.3684)  loss_classifier: 0.6806 (0.6806)  loss_box_reg: 0.2409 (0.2409)  loss_mask: 1.8763 (1.8763)  loss_objectness: 0.5563 (0.5563)  loss_rpn_box_reg: 0.0143 (0.0143)  time: 8.5074  data: 0.0236\n",
      "Epoch: [0]  [4/5]  eta: 0:00:05  lr: 0.001000  loss: 1.3856 (1.9998)  loss_classifier: 0.3450 (0.3878)  loss_box_reg: 0.1434 (0.1452)  loss_mask: 1.0227 (1.2392)  loss_objectness: 0.0379 (0.2182)  loss_rpn_box_reg: 0.0089 (0.0094)  time: 5.7571  data: 0.0215\n",
      "Epoch: [0] Total time: 0:00:28 (5.7576 s / it)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfortuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pedestrians_dirpath\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m ObjectDetectionModel(n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fortuna/src/fortuna/_ObjectDetectionModel.py:80\u001b[0m, in \u001b[0;36mObjectDetectionModel.fit\u001b[0;34m(self, training_dataset, validation_dataset, n_epochs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m     71\u001b[0m     train_one_epoch(\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     73\u001b[0m         optimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m         print_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     78\u001b[0m     )\n\u001b[0;32m---> 80\u001b[0m     \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fortuna/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fortuna/src/fortuna/torchutils/engine.py:85\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     82\u001b[0m metric_logger \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mMetricLogger(delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m header \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 85\u001b[0m coco \u001b[38;5;241m=\u001b[39m \u001b[43mget_coco_api_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m iou_types \u001b[38;5;241m=\u001b[39m _get_iou_types(model)\n\u001b[1;32m     87\u001b[0m coco_evaluator \u001b[38;5;241m=\u001b[39m CocoEvaluator(coco, iou_types)\n",
      "File \u001b[0;32m~/fortuna/src/fortuna/torchutils/coco_utils.py:206\u001b[0m, in \u001b[0;36mget_coco_api_from_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCocoDetection):\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcoco\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_coco_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fortuna/src/fortuna/torchutils/coco_utils.py:155\u001b[0m, in \u001b[0;36mconvert_to_coco_api\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m    151\u001b[0m categories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ds)):\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# find better way to get target\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# targets = ds.get_annotations(img_idx)\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     img, targets \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    156\u001b[0m     image_id \u001b[38;5;241m=\u001b[39m targets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    157\u001b[0m     img_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/fortuna/src/fortuna/_ObjectDetectionDataset.py:87\u001b[0m, in \u001b[0;36mObjectDetectionDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     84\u001b[0m image, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_transforms(image, target)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Apply user-passed transforms.\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m image, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "File \u001b[0;32m~/fortuna/src/fortuna/torchutils/transforms.py:25\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, image, target)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, target):\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m     26\u001b[0m         image, target \u001b[38;5;241m=\u001b[39m t(image, target)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Initialize and fit the object detector.\n",
    "\n",
    "from fortuna import ObjectDetectionModel\n",
    "from fortuna.data import pedestrians_dirpath\n",
    "\n",
    "model = ObjectDetectionModel(n_classes=2)\n",
    "\n",
    "model.fit(\n",
    "    training_dataset,\n",
    "    validation_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the detector.\n",
    "\n",
    "model.evaluate(\n",
    "    validation_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model.\n",
    "\n",
    "model.write(\"path_to_model_file.onnx\") # Export as e.g. ONNX model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3fa1d062a45f38cd6e6e990d694ac8adf2db69accfdb7573edc8f68c95dca34a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
