{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 230129 - TorchVision Object Detection Tutorial\n",
    "\n",
    "[Object detection tutorial link](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)  \n",
    "[Image labelling tool link](https://labelstud.io/)\n",
    "\n",
    "In Docker: \n",
    "```\n",
    "docker run -it -p 8080:8080 -v ${pwd}/mydata:/label-studio/data heartexlabs/label-studio:latest\n",
    "```\n",
    "May need\n",
    "```\n",
    "export DOCKER_DEFAULT_PLATFORM=linux/amd64\n",
    "```\n",
    "before this to enable architecture emulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision\n",
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Dataset\n",
    "\n",
    "[Download this with `wget` and extract it with `unzip`](https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip).\n",
    "\n",
    "Then write a `torch.utils.data.Dataset` for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from detection.engine import train_one_epoch, evaluate\n",
    "from detection.utils import collate_fn\n",
    "import detection.transforms as T\n",
    "\n",
    "def bbox_from_mask(mask: np.ndarray):\n",
    "    # boolean mask has shape (height, width)\n",
    "    pos = np.where(mask)\n",
    "    xmin = np.min(pos[1])\n",
    "    xmax = np.max(pos[1])\n",
    "    ymin = np.min(pos[0])\n",
    "    ymax = np.max(pos[0])\n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.image_paths = sorted((Path(root) / \"PNGImages\").rglob(\"*\"))\n",
    "        self.mask_paths = sorted((Path(root) / \"PedMasks\").rglob(\"*\"))\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # Get image and mask paths.\n",
    "        image_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        \n",
    "        # Load image.\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Load mask. NB object_ids denote distinct objects, not classes.\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = np.array(mask)\n",
    "        object_ids = np.unique(mask) # Get object ids from distinct values in mask.\n",
    "        object_ids = object_ids[1:] # First id (0) is the background, so remove it.\n",
    "        masks = (mask == object_ids[:, None, None]) # Each channel of masks corresponds to a different object id.\n",
    "\n",
    "        # Extract bounding box coordinates from each segmentation mask.\n",
    "        n_objects = len(object_ids)\n",
    "        bboxes = [bbox_from_mask(masks[i, :, :]) for i in range(n_objects)]\n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)\n",
    "\n",
    "        # Package into dict.\n",
    "        target = {\n",
    "            \"boxes\": bboxes, # Bounding boxes.\n",
    "            \"labels\": torch.ones((n_objects,), dtype=torch.int64), # Class labels (only one class for PennFudan).\n",
    "            \"masks\": torch.as_tensor(masks, dtype=torch.uint8), # Segmentation masks.\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0]),\n",
    "            \"is_crowd\": torch.zeros((n_objects,), dtype=torch.int64) # Set iscrowd = False for all.\n",
    "        }\n",
    "\n",
    "        # Apply transformations if applicable.\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    \n",
    "def get_dataloaders() -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
    "    # Returns training and testing data loaders.\n",
    "    \n",
    "    # Load dataset.\n",
    "    num_classes = 2 # Dataset only has background and person classes.\n",
    "    dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "    dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "    \n",
    "    # Create train/test splits.\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "    # Training and testing loaders.\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return data_loader, data_loader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we'll be using is Mask-RCNN, which is like Faster-RCNN except it also outputs a segmentation mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes: int) \\\n",
    "    -> torchvision.models.detection.mask_rcnn.MaskRCNN:\n",
    "    \"\"\"\n",
    "    num_classes includes background class e.g. PennFudan has\n",
    "    two classes - background and person.\n",
    "    \n",
    "    model.__call__ \n",
    "    * List[FloatTensor] -> returns List[Dict] where each Dict has keys \"boxes\", \"labels\", \"scores\".\n",
    "    * FloatTensors can be different sizes, must be three channels.\n",
    "    * Gradients are attached to \"boxes\", \"labels\", \"scores\" tensors.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Load an instance segmentation model pre-trained on COCO.\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # Replace the pre-trained box predictor head with a new one.\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features    \n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Replace the pre-trained mask predictor head with a new one.\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        in_channels = in_features_mask, # Number of input channels.\n",
    "        dim_reduced = 256, # Number of hidden layer units.\n",
    "        num_classes = num_classes # Number of object classes.\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need the helper functions in `references/detection` [here](https://github.com/pytorch/vision). Clone the repo and copy the `detection` dir to the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detection.transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    # Returns transform used in model.\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "        \n",
    "    # Images are not resized?\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_and_lr_scheduler(params: List[torch.nn.parameter.Parameter]):\n",
    "    optimizer = torch.optim.SGD(\n",
    "        params,\n",
    "        lr=0.005,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0005\n",
    "    )\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=3,\n",
    "        gamma=0.1\n",
    "    )\n",
    "    return optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create main function which performs the training and validation.\n",
    "\n",
    "def main():\n",
    "    # Config.\n",
    "    num_classes = 2\n",
    "    num_epochs = 10\n",
    "    \n",
    "    # Train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Get data loaders.\n",
    "    data_loader, data_loader_test = get_dataloaders()\n",
    "    \n",
    "    # Get the model.\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Get optimizer.\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer, lr_scheduler = get_optimizer_and_lr_scheduler(params)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train for one epoch, printing every 10 iterations.\n",
    "        train_one_epoch(\n",
    "            model,\n",
    "            optimizer,\n",
    "            data_loader,\n",
    "            device,\n",
    "            epoch,\n",
    "            print_freq=10\n",
    "        )\n",
    "        \n",
    "        # Update the learning rate.\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Evaluate on the test dataset.\n",
    "        evaluate(\n",
    "            model,\n",
    "            data_loader_test,\n",
    "            device=device\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/60]  eta: 0:17:21  lr: 0.000090  loss: 5.6852 (5.6852)  loss_classifier: 0.6384 (0.6384)  loss_box_reg: 0.3683 (0.3683)  loss_mask: 4.6103 (4.6103)  loss_objectness: 0.0614 (0.0614)  loss_rpn_box_reg: 0.0068 (0.0068)  time: 17.3612  data: 0.3620\n",
      "Epoch: [0]  [10/60]  eta: 0:11:57  lr: 0.000936  loss: 1.5781 (2.7810)  loss_classifier: 0.4137 (0.4127)  loss_box_reg: 0.2194 (0.2163)  loss_mask: 0.9623 (2.1290)  loss_objectness: 0.0136 (0.0184)  loss_rpn_box_reg: 0.0050 (0.0045)  time: 14.3515  data: 0.0379\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
